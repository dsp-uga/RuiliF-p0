{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:45533</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>16</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>16.42 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:45533' processes=16 threads=16, memory=16.42 GB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=16)\n",
    "\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "from dask import bag as db\n",
    "\n",
    "#throw all the words into one big bag, case sensitive\n",
    "#b = db.read_text(os.path.join('data', '*.txt'))\n",
    "#b.take(100) \n",
    "\n",
    "#'''\n",
    "##preprocess the files, put all the content info one file 'out.tmp' (now find a better way)\n",
    "#outfile = open('out.tmp', 'w')\n",
    "#'''\n",
    "filelist = []\n",
    "for file in glob.glob('./data/*.txt'):\n",
    "    filelist.append(file)\n",
    "\n",
    "#    linecount = 1\n",
    "#    with open(file, 'r', encoding='utf-8-sig') as f:\n",
    "#        for line in f:\n",
    "#            words = line.strip().split()\n",
    "#            for word in words:\n",
    "#                outfile.write('%s\\n' % (word.lower()))\n",
    "\n",
    "#outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#throw all the words into one big bag\n",
    "b = db.read_text(os.path.join('data', '*.txt'), encoding='utf-8-sig').map(lambda x:x.casefold().strip().split()).flatten()\n",
    "\n",
    "#generate output\n",
    "newbag = b.frequencies(sort=True) #sort\n",
    "dic = dict(newbag.take(40)) #get top 40\n",
    "\n",
    "with open(\"sp1.json\", \"w\") as outfile:\n",
    "    json.dump(dic, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open('stopwords.txt') as f:\n",
    "    for word in f:\n",
    "        stop_words.append(word.strip())\n",
    "\n",
    "def is_stopwords(w):\n",
    "    return w in stop_words\n",
    "\n",
    "#no-stop-words-bag. filter out all the stop words\n",
    "nstpbg = b.remove(is_stopwords)\n",
    "\n",
    "#generate output\n",
    "newbag = nstpbg.frequencies(sort=True)\n",
    "dic = dict(newbag.take(40))\n",
    "\n",
    "with open(\"sp2.json\", \"w\") as outfile:\n",
    "    json.dump(dic, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discard all words that have only 1 character\n",
    "def is_1char(w):\n",
    "    return len(w) == 1\n",
    "\n",
    "#n1cbg. discard 1-character words\n",
    "n1cbg = nstpbg.remove(is_1char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip the punctuation\n",
    "punc_list=[',', '.', '!', '?', ':', ';', \"'\"]\n",
    "def trimp(word):\n",
    "    if word[0] in punc_list:\n",
    "        word = word[1:]\n",
    "    if word[-1] in punc_list:\n",
    "        word = word[:-1]\n",
    "    return word\n",
    "\n",
    "npbg = n1cbg.map(trimp)\n",
    "\n",
    "#generate output\n",
    "newbag = npbg.frequencies(sort = True)\n",
    "dic = dict(newbag.take(40))\n",
    "\n",
    "with open(\"sp3.json\", \"w\") as outfile:\n",
    "    json.dump(dic, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#import tlz as toolz\n",
    "from tlz import second\n",
    "\n",
    "baglist = []\n",
    "tf_idf_list = []\n",
    "\n",
    "outdict = {}\n",
    "for file in filelist:\n",
    "    #print(file)\n",
    "    bag = db.read_text(file, encoding='utf-8-sig').map(lambda x:x.strip().split()).flatten().str.casefold()\n",
    "    bag = bag.remove(is_stopwords).remove(is_1char)\n",
    "    bag = bag.map(trimp)\n",
    "    #print(bag.take(10))\n",
    "    baglist.append(bag)\n",
    "    \n",
    "assert(len(baglist)==8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this part takes a thousand years ... ...\n",
    "outdict = {}\n",
    "\n",
    "def get_idf(word):\n",
    "    nt = 0\n",
    "    for bag in baglist:\n",
    "        #check if the word exists in this file\n",
    "        #this is definitely a lame implementation, while I'm not able to find a better way to do it\n",
    "        \n",
    "        newbag = bag.map(lambda x: x == word)\n",
    "        #only '{False ***}' or 'True ***, False ***'\n",
    "        if newbag.distinct().count().compute()>1:\n",
    "            nt += 1\n",
    "    if nt == 0:\n",
    "        # nt should only equal or larger than 1\n",
    "        assert(nt !=0), \"word = %s\" % (word)\n",
    "        return 0\n",
    "    else:\n",
    "        return math.log(8/nt)\n",
    "\n",
    "for bag in baglist:\n",
    "    tfbag = bag.frequencies(sort = False)\n",
    "    tf_idf_bag = tfbag.map(lambda x:(x[0], x[1]*get_idf(x[0])))\n",
    "    #print('get tf_idf')\n",
    "    #sort by value\n",
    "    tf_idf_bag = tf_idf_bag.map_partitions(sorted, key=second, reverse = True)\n",
    "    #print('tf_idf sorted')\n",
    "    #tf_idf_list.append(tf_idf_bag) <-- no longer needed. already add to outdict\n",
    "    \n",
    "    outdict={**outdict, **dict(tf_idf_bag.take(5))}\n",
    "\n",
    "with open(\"sp4.json\", \"w\") as outfile:\n",
    "    json.dump(outdict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "'''\n",
    "diction = {}\n",
    "from tlz import second\n",
    "testb = db.from_sequence(['ef','aaa','aaa','c','d','a', 'd'])\n",
    "testb = testb.frequencies(sort = False)\n",
    "testb = testb.map(lambda x:(x[0], x[1]*1.5))\n",
    "testb = testb.map_partitions(sorted, key=second, reverse = True)\n",
    "print(dict(testb))\n",
    "#print(testb.compute())\n",
    "#diction = Merge(diction, dict(testb))\n",
    "diction={**diction, **(dict(testb))}\n",
    "print(diction)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "init = 0\n",
    "word = 'f'\n",
    "def count(x, y):\n",
    "    return (x==word or y == word)\n",
    "testb = db.from_sequence(['a', 'b', 'c', 'd', 'a'])\n",
    "print(testb.fold(count).compute())\n",
    "print(testb.count().compute())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(outdict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
